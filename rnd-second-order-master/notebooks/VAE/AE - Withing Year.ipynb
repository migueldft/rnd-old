{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/calsaverini/dev/analysis/survival/second_order/VAE/venv/lib/python3.6/site-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pycm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics as skmetrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
    "from sklearn.preprocessing import label_binarize, QuantileTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.utils import use_named_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"./dataset.parquet\").sample(5000)\n",
    "categorical_columns = ['partner', 'device', 'gender', 'state', 'channel']\n",
    "feature_columns = [\n",
    "    'channel', 'partner', 'device', 'age', 'gender', 'state', 'has_marketplace', \n",
    "    'has_crossdocking', 'has_private_label', 'has_brands', 'gmv', 'fst_sale_in_black_friday_days', \n",
    "    'snd_sale_in_black_friday_days'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=categorical_columns, drop_first=True)\n",
    "df = df.loc[df.waiting_time > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [c for c in df.columns if any([c.startswith(x) for x in feature_columns])]\n",
    "X = df.loc[:, features]\n",
    "y = df.loc[:, 'has_second_sale_within_year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "qt = QuantileTransformer()\n",
    "qt.fit(X_train.loc[:, [\"age\", \"gmv\"]])\n",
    "\n",
    "X_train.loc[:, [\"age\", \"gmv\"]] = qt.transform(X_train.loc[:, [\"age\", \"gmv\"]])\n",
    "X_test.loc[:, [\"age\", \"gmv\"]] = qt.transform(X_test.loc[:, [\"age\", \"gmv\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPERPARAMETER_SPACE = [\n",
    "    Integer(1, 10, name='min_child_weight'),\n",
    "    Real(1e-5, 1.0, \"uniform\", name='learning_rate'),\n",
    "    Real(0.1, 10, \"log-uniform\", name='gamma'),    \n",
    "    Real(0.05, 1, name='subsample'),\n",
    "    Real(0.05, 1, name='colsample_bytree'),\n",
    "#     Integer(2, 6, name='max_depth'),\n",
    "#     Integer(100, 200, name=\"n_estimators\"),\n",
    "    Real(0.01, 0.99, name=\"base_score\"),\n",
    "#     Real(0.9, 1,0, name=\"scale_pos_weight\"),\n",
    "    Real(0, 5, name=\"reg_alpha\"),\n",
    "    Real(0, 5, name=\"reg_lambda\"),\n",
    "]\n",
    "\n",
    "HYPERPARAMETER_NAMES = [dim.name for dim in HYPERPARAMETER_SPACE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 939568576\n",
    "TREE_METHOD = 'hist'#'exact'\n",
    "NUM_PROCESSORS = 8\n",
    "NUM_CROSS_VALIDATION_STEPS = 1\n",
    "NUM_GP_OPTIMIZATION_STEPS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logger(name):\n",
    "    logger = logging.getLogger(name)\n",
    "    formatter = logging.Formatter('[%(asctime)s] %(message)s')\n",
    "    stream_handler = logging.StreamHandler()\n",
    "    stream_handler.setFormatter(formatter)\n",
    "    logger.addHandler(stream_handler)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    return logger\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, y, **kwargs):\n",
    "    model = XGBClassifier(\n",
    "        max_depth=5,\n",
    "        n_estimators=1000,\n",
    "        scale_pos_weight=1,\n",
    "        random_state=RANDOM_STATE,\n",
    "        tree_method=TREE_METHOD,\n",
    "        n_jobs=NUM_PROCESSORS,\n",
    "        **kwargs,\n",
    "    )\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "def evaluate_model(X, y, **kwargs):\n",
    "    roc_auc = []\n",
    "    for _ in range(NUM_CROSS_VALIDATION_STEPS):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "        model = train_model(X_train, y_train, **kwargs)\n",
    "        score = model.predict_proba(X_test)\n",
    "        #total_correlation = np.sqrt(1 - np.exp(-2 * mutual_info_classif(score, y_test)))\n",
    "        log_like = np.log(score[range(y_test.size), y_test]).mean()\n",
    "        roc_auc.append(log_like)\n",
    "    logger.info(f\"Values: {roc_auc}\")\n",
    "    return np.mean(roc_auc)\n",
    "\n",
    "\n",
    "def train_optimized_model(X, y):\n",
    "        \n",
    "    @use_named_args(HYPERPARAMETER_SPACE)\n",
    "    def loss(**kwargs):\n",
    "        return -evaluate_model(X, y, **kwargs)\n",
    "\n",
    "    def get_optimization_callback():\n",
    "        message = \"Hyperparameter optimization iteration {:d}/{:d}. Current: {:5.3f}. Best: {:5.3f}.\"\n",
    "            #\" Parameters:\\n{}\"\n",
    "        def callback(res):          \n",
    "            current_value = -res.func_vals[-1]\n",
    "            best_value = -res.func_vals.min()            \n",
    "            current_params = pd.Series(dict(zip(\n",
    "                HYPERPARAMETER_NAMES, \n",
    "                map(fix_type, res.x)\n",
    "            )))\n",
    "            logger.info(message.format(\n",
    "                len(res.func_vals),\n",
    "                NUM_GP_OPTIMIZATION_STEPS,\n",
    "                current_value,\n",
    "                best_value,\n",
    "                #current_params\n",
    "            ))\n",
    "\n",
    "        return callback\n",
    "\n",
    "    def fix_type(x):\n",
    "        if isinstance(x, bool):\n",
    "            return x\n",
    "        elif np.issubdtype(np.dtype(x), np.int_):\n",
    "            return int(x)\n",
    "        elif np.issubdtype(np.dtype(x), np.float_):\n",
    "            return float(x)\n",
    "\n",
    "    logger.info(\"Will start hyperparameter optimization.\")\n",
    "    optimization_results = gp_minimize(\n",
    "        loss,\n",
    "        HYPERPARAMETER_SPACE,\n",
    "        n_random_starts=max(1, min(10, int(NUM_GP_OPTIMIZATION_STEPS / 2))),\n",
    "        n_calls=NUM_GP_OPTIMIZATION_STEPS,\n",
    "        random_state=RANDOM_STATE,\n",
    "        callback=get_optimization_callback(),\n",
    "    )\n",
    "    logger.info(\"Finished hyperparameter optimization.\")\n",
    "    best_parameters = dict(\n",
    "        zip(HYPERPARAMETER_NAMES, map(fix_type, optimization_results.x))\n",
    "    )\n",
    "    logger.info(\"Starting training of final model.\")\n",
    "    return train_model(X, y, verbosity=1, **best_parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%time best_model = train_optimized_model(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(X_test)\n",
    "s_pred = best_model.predict_proba(X_test)\n",
    "\n",
    "print(skmetrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = pycm.ConfusionMatrix(actual_vector=y_test.values, predict_vector=y_pred)\n",
    "cm.print_normalized_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, roc_thres = skmetrics.roc_curve(y_test, s_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 4))\n",
    "\n",
    "ax = fig.add_subplot(121)\n",
    "ax.set_title(\"ROC Curve\")\n",
    "ax.plot(fpr, tpr)\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "ax.set_title(\"Positive Rates\")\n",
    "ax.plot(roc_thres, tpr, label=\"TPR\")\n",
    "ax.plot(roc_thres, fpr, label=\"FPR\")\n",
    "ax.set_xlim(0, 1)\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_correlation = np.sqrt(1 - np.exp(-2 * mutual_info_classif(s_pred, y_test))).min()\n",
    "print(total_correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skmetrics.roc_auc_score(label_binarize(y_test, range(len(code2period))), s_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skmetrics.roc_auc_score(label_binarize(y_test, range(len(code2period))), s_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = min(100, np.unique(s_pred).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "thresholds = np.linspace(s_pred.min(), s_pred.max()*0.999, n_points)\n",
    "\n",
    "metrics = pd.DataFrame([\n",
    "    skmetrics.precision_recall_fscore_support(y_test, (s_pred >= threshold), average='binary')\n",
    "    for threshold in thresholds\n",
    "], columns=[\"precision\", \"recall\", \"f1\", \"support\"], index=thresholds)\n",
    "\n",
    "optimal_threshold = metrics.f1.argmax()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 4))\n",
    "\n",
    "ax = fig.add_subplot(121)\n",
    "ax.set_title(\"Precision x Recall Curve\")\n",
    "ax.plot(metrics.precision, metrics.recall)\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "ax.set_title(\"Positive Rates\")\n",
    "ax.plot(metrics.index, metrics.precision, label=\"Precision\")\n",
    "ax.plot(metrics.index, metrics.recall, label=\"Recall\")\n",
    "ax.plot(metrics.index, metrics.f1, label=\"F_1 Score\")\n",
    "ax.axvline(optimal_threshold)\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_threshold = metrics.f1.argmax()\n",
    "print(metrics.loc[optimal_threshold])\n",
    "y_pred = (s_pred > optimal_threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(skmetrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save_artifact(\"./within_year_model.pred\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
