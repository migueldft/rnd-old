{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    'date': 'object',\n",
    "    'customer_id': 'object',\n",
    "    'n_orders': 'int',\n",
    "    'customer_gender': 'object',\n",
    "    'customer_age': 'int',\n",
    "    'customer_created_at': 'object',\n",
    "    'customer_first_order_paid': 'object',\n",
    "    'channel_partner_name': 'object',\n",
    "    'sku_config': 'object',\n",
    "    'is_campaign': 'bool',\n",
    "    'product_name': 'object',\n",
    "    'product_gender': 'object',\n",
    "    'product_color': 'object',\n",
    "    'product_brand': 'object',\n",
    "    'product_medium_image_url': 'object',\n",
    "    'cmc_category_bp': 'object', \n",
    "    'cmc_division_bp': 'object',\n",
    "    'cmc_business_unit_bp': 'object',\n",
    "    'google_product_category': 'object',\n",
    "    'product_original_price': 'float',\n",
    "    'payment_method_name': 'object',\n",
    "    'shipping_condition': 'object',\n",
    "    'product_discount': 'object',\n",
    "    'shipping_discount': 'object',\n",
    "    'sale_value': 'float',\n",
    "    'is_coupon': 'bool',\n",
    "    'device_name': 'object',\n",
    "    'platform_name': 'object',\n",
    "    'delivery_city': 'object',\n",
    "    'delivery_state_code': 'object',\n",
    "    'delivery_country_region': 'object',\n",
    "    'planning_cluster': 'object',\n",
    "    'planning_age': 'object',\n",
    "    'ticketrange_planning': 'object',\n",
    "    'originalpricerange_planning': 'object'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dask import delayed\n",
    "\n",
    "@delayed\n",
    "def load(filename):\n",
    "    return pd.read_parquet(filename, engine='pyarrow')\n",
    "\n",
    "@delayed\n",
    "def clean(data):\n",
    "    return data\n",
    "\n",
    "@delayed\n",
    "def analyze(sequence_of_data):\n",
    "    pass\n",
    "\n",
    "@delayed\n",
    "def store(result):\n",
    "    with open(..., 'w') as f:\n",
    "        f.write(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = Path(\".\").glob(DATA_PATH)\n",
    "\n",
    "loaded = dd.DataFrame([load(i) for i in files])\n",
    "cleaned = [clean(i) for i in loaded]\n",
    "analyzed = analyze(cleaned)\n",
    "stored = store(analyzed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../test_dir/input/data/training/part*parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import lens\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_parquet(DATA_PATH, columns=[\"product_medium_image_url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method to_parquet in module dask.dataframe.core:\n",
      "\n",
      "to_parquet(path, *args, **kwargs) method of dask.dataframe.core.DataFrame instance\n",
      "    Store Dask.dataframe to Parquet files\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    Each partition will be written to a separate file.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    df : dask.dataframe.DataFrame\n",
      "    path : string\n",
      "        Destination directory for data.  Prepend with protocol like ``s3://``\n",
      "        or ``hdfs://`` for remote data.\n",
      "    engine : {'auto', 'fastparquet', 'pyarrow'}, default 'auto'\n",
      "        Parquet library to use. If only one library is installed, it will use\n",
      "        that one; if both, it will use 'fastparquet'.\n",
      "    compression : string or dict, optional\n",
      "        Either a string like ``\"snappy\"`` or a dictionary mapping column names\n",
      "        to compressors like ``{\"name\": \"gzip\", \"values\": \"snappy\"}``. The\n",
      "        default is ``\"default\"``, which uses the default compression for\n",
      "        whichever engine is selected.\n",
      "    write_index : boolean, optional\n",
      "        Whether or not to write the index. Defaults to True *if* divisions are\n",
      "        known.\n",
      "    append : bool, optional\n",
      "        If False (default), construct data-set from scratch. If True, add new\n",
      "        row-group(s) to an existing data-set. In the latter case, the data-set\n",
      "        must exist, and the schema must match the input data.\n",
      "    ignore_divisions : bool, optional\n",
      "        If False (default) raises error when previous divisions overlap with\n",
      "        the new appended divisions. Ignored if append=False.\n",
      "    partition_on : list, optional\n",
      "        Construct directory-based partitioning by splitting on these fields'\n",
      "        values. Each dask partition will result in one or more datafiles,\n",
      "        there will be no global groupby.\n",
      "    storage_options : dict, optional\n",
      "        Key/value pairs to be passed on to the file-system backend, if any.\n",
      "    compute : bool, optional\n",
      "        If True (default) then the result is computed immediately. If False\n",
      "        then a ``dask.delayed`` object is returned for future computation.\n",
      "    **kwargs\n",
      "        Extra options to be passed on to the specific backend.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = dd.read_csv(...)  # doctest: +SKIP\n",
      "    >>> dd.to_parquet(df, '/path/to/output/', compression='snappy')  # doctest: +SKIP\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    read_parquet: Read parquet data to dask.dataframe\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(df.to_parquet, engine='pyarrow', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('test', partition_on=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ids = unique_url.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can only summarise a Pandas DataFrame",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-26b94ac16efd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummarise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/jupyter3/lib/python3.7/site-packages/lens/summarise.py\u001b[0m in \u001b[0;36msummarise\u001b[0;34m(df, scheduler, num_workers, size, pairdensities)\u001b[0m\n\u001b[1;32m    835\u001b[0m     \"\"\"\n\u001b[1;32m    836\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 837\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can only summarise a Pandas DataFrame\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    838\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can only summarise a Pandas DataFrame"
     ]
    }
   ],
   "source": [
    "ls = lens.summarise(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_id = df.customer_id.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dd.Scalar<series-..., dtype=int64>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.customer_id.nunique().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44     113751\n",
       "45     106992\n",
       "46      96678\n",
       "47      95372\n",
       "48      87765\n",
       "49      87665\n",
       "50      85496\n",
       "51      81521\n",
       "52      76342\n",
       "54      72305\n",
       "53      72055\n",
       "55      71065\n",
       "56      63404\n",
       "58      60705\n",
       "57      60027\n",
       "61      58396\n",
       "59      56529\n",
       "60      55745\n",
       "62      54535\n",
       "64      50506\n",
       "63      49273\n",
       "65      44717\n",
       "66      44713\n",
       "69      44093\n",
       "68      42123\n",
       "70      41963\n",
       "67      39645\n",
       "74      38039\n",
       "79      36553\n",
       "80      35300\n",
       "        ...  \n",
       "310       528\n",
       "257       519\n",
       "206       507\n",
       "243       458\n",
       "428       449\n",
       "554       436\n",
       "372       433\n",
       "214       430\n",
       "270       428\n",
       "377       417\n",
       "313       416\n",
       "197       394\n",
       "233       392\n",
       "306       374\n",
       "319       365\n",
       "247       345\n",
       "286       329\n",
       "225       319\n",
       "261       276\n",
       "367       265\n",
       "318       250\n",
       "349       211\n",
       "395       190\n",
       "314       181\n",
       "458       171\n",
       "237       145\n",
       "347        86\n",
       "293        83\n",
       "405        81\n",
       "300        37\n",
       "Name: n_orders, Length: 313, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "df.n_orders.value_counts().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3534847, 35)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of datapoints: 3534847\n",
      "\n",
      "## Integer Columns\n",
      "Column name                : Non-nan  mean   std   min   25%   50%   75%   max\n",
      "\n",
      "## Float Columns\n",
      "Column name                : Non-nan   mean    std    min    25%    50%    75%    max\n",
      "product_original_price     : 3534847  656.51  219299.42   4.00  79.00  130.00  229.00  99999999.00\n",
      "product_discount           : 3534847  10.45  25.08   0.00   0.00   0.00   9.10  1750.49\n",
      "shipping_discount          : 3534847  1278.50  342256.24  -55.98   1.41   3.19   6.19  99999999.99\n",
      "sale_value                 : 3534847  100.17  83.14   0.00  44.99  76.99  130.19  3998.00\n",
      "\n",
      "## Category Columns\n",
      "Column name                : Non-nan   unique   top (count)  rest\n",
      "customer_id                : 3534847    23664   1445343 (742)  ['1601186', '1810075', '1997117', '21173\n",
      "customer_gender            : 3531028        3   female (2996744)  ['male']\n",
      "channel_partner_name       : 3534847       74   APPLIFT (46938)  ['AUTOMATED', 'BING', 'CRITEO', 'DIRECT'\n",
      "sku_config                 : 3534847   689837   10173APF00DKH (2)  ['10173APF00FHF', '10173APF18PKF', '1017\n",
      "product_gender             : 3534847        5   feminino (2507425)  ['masculino', 'menina', 'menino', 'uniss\n",
      "product_color              : 3534847      617   Amarelo (82638)  ['Amarelo/Rosa', 'Azul', 'Azul Marinho',\n",
      "product_brand              : 3534847      640   #MO (5344)  ['101 Resort Wear', 'AHA', 'AMARO', 'AMB\n",
      "cmc_category_bp            : 3534847       62   Acessórios (281)  ['Bermuda/Shorts', 'Bermuda/Shorts/Saia'\n",
      "cmc_division_bp            : 3534847       13   Kids Apparel (266949)  ['Kids Shoes', 'Men Accessories', 'Men A\n",
      "cmc_business_unit_bp       : 3534847        5   Kids and Home (377670)  ['Men', 'Sports', 'Women Apparel and Bea\n",
      "google_product_category    : 3534847       30   Artigos esportivos > Artigos para prática de esportes > Futebol (337)  ['Malas e bolsas > Mochilas', 'Malas e b\n",
      "payment_method_name        : 3534847        6   BRASPAG_BOLETO (794240)  ['BRASPAG_CC', 'DEBITCARD', 'NO_PAYMENT'\n",
      "shipping_condition         : 3534847        8   ECONOMIC (37564)  ['EXPRESS', 'NORMAL', 'TURBO DAY ZERO', \n",
      "device_name                : 3534847        5   DESKTOP (1783211)  ['MOBILE', 'TABLET', 'TELESALES', 'UNKNO\n",
      "platform_name              : 3534847        3   APP ANDROID (782034)  ['APP IOS', 'BROWSER']\n",
      "delivery_city              : 2876172     3158   Altinho (1772)  ['Arapongas', 'Brasilia', 'Curitiba', 'H\n",
      "delivery_state_code        : 2876172       28   DF (109225)  ['MG', 'PE', 'PR', 'SP', 'RJ', 'RS', 'SC\n",
      "delivery_country_region    : 2876172        6   Centro-Oeste (276303)  ['Nordeste', 'Sudeste', 'Sul', 'Norte']\n",
      "planning_cluster           : 3534847       28   ALMA CARIOCA (208635)  ['CASUAL', 'CASUAL/BÁSICO', 'CLASSICA', \n",
      "planning_age               : 3502912        6   Adulto (919319)  ['Infantil', 'Jovem Adulto', 'Sênior', '\n",
      "ticketrange_planning       : 3534834        5   T1 (1801541)  ['T2', 'T3', 'T4']\n",
      "originalpricerange_planning: 3534834        5   O1 (1501891)  ['O2', 'O3', 'O4']\n",
      "\n",
      "## Other Columns\n",
      "Column name                : Non-nan   unique   top (count)\n",
      "is_campaign                : 3534847        2   0 (3143110)\n",
      "is_coupon                  : 3534847        2   0 (2476761)\n"
     ]
    }
   ],
   "source": [
    "# core modules\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s',\n",
    "                    level=logging.DEBUG,\n",
    "                    stream=sys.stdout)\n",
    "\n",
    "\n",
    "def describe_pandas_df(df, dtype=None):\n",
    "    \"\"\"\n",
    "    Show basic information about a pandas dataframe.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : Pandas Dataframe object\n",
    "    dtype : dict\n",
    "        Maps column names to types\n",
    "    \"\"\"\n",
    "    if dtype is None:\n",
    "        dtype = {}\n",
    "    print(\"Number of datapoints: {datapoints}\".format(datapoints=len(df)))\n",
    "    column_info = {'int': [], 'float': [], 'category': [], 'other': []}\n",
    "    float_types = ['float64']\n",
    "    integer_types = ['int64', 'uint8']\n",
    "    other_types = ['object', 'category']\n",
    "    column_info_meta = {}\n",
    "    for column_name in df:\n",
    "        column_info_meta[column_name] = {}\n",
    "        counter_obj = df[column_name].groupby(df[column_name]).count()\n",
    "        value_list = list(counter_obj.keys())\n",
    "        value_count = len(value_list)\n",
    "        is_suspicious_cat = (value_count <= 50 and\n",
    "                             str(df[column_name].dtype) != 'category' and\n",
    "                             column_name not in dtype)\n",
    "        if is_suspicious_cat:\n",
    "            logging.warning(\"Column '{}' has only {} different values ({}). \"\n",
    "                            \"You might want to make it a 'category'\"\n",
    "                            .format(column_name,\n",
    "                                    value_count,\n",
    "                                    value_list))\n",
    "        top_count_val = counter_obj[value_list[0]]\n",
    "        column_info_meta[column_name]['top_count_val'] = top_count_val\n",
    "        column_info_meta[column_name]['value_list'] = value_list\n",
    "        column_info_meta[column_name]['value_count'] = value_count\n",
    "\n",
    "        if df[column_name].dtype in integer_types:\n",
    "            column_info['int'].append(column_name)\n",
    "        elif df[column_name].dtype in float_types:\n",
    "            column_info['float'].append(column_name)\n",
    "        elif str(df[column_name].dtype) == 'category':\n",
    "            column_info['category'].append(column_name)\n",
    "        elif str(df[column_name].dtype) in other_types:\n",
    "            column_info['other'].append(column_name)\n",
    "        else:\n",
    "            print(\"!!! describe_pandas_df does not know type '{}'\"\n",
    "                  .format(df[column_name].dtype))\n",
    "\n",
    "    column_name_len = max(len(column_name) for column_name in df)\n",
    "\n",
    "    print(\"\\n## Integer Columns\")\n",
    "    print(\"{column_name:<{column_name_len}}: Non-nan  mean   std   min   25%  \"\n",
    "          \" 50%   75%   max\"\n",
    "          .format(column_name_len=column_name_len,\n",
    "                  column_name=\"Column name\"))\n",
    "    for column_name in column_info['int']:\n",
    "        print(\"{column_name:<{column_name_len}}: {non_nan:>7}  \"\n",
    "              \"{mean:0.2f}  {std:>4.2f}  \"\n",
    "              \"{min:>4.0f}  {q25:>4.0f}  {q50:>4.0f}  {q75:>4.0f}  {max:>4.0f}\"\n",
    "              .format(column_name_len=column_name_len,\n",
    "                      column_name=column_name,\n",
    "                      non_nan=sum(df[column_name].notnull()),\n",
    "                      mean=df[column_name].mean(),\n",
    "                      std=df[column_name].std(),\n",
    "                      min=df[column_name].min(),\n",
    "                      q25=df[column_name].quantile(0.25),\n",
    "                      q50=df[column_name].quantile(0.50),\n",
    "                      q75=df[column_name].quantile(0.75),\n",
    "                      max=max(df[column_name])))\n",
    "\n",
    "    print(\"\\n## Float Columns\")\n",
    "    print(\"{column_name:<{column_name_len}}: Non-nan   mean    std    min    \"\n",
    "          \"25%    50%    75%    max\"\n",
    "          .format(column_name_len=column_name_len,\n",
    "                  column_name=\"Column name\"))\n",
    "    for column_name in column_info['float']:\n",
    "        print(\"{column_name:<{column_name_len}}: {non_nan:>7}  \"\n",
    "              \"{mean:5.2f}  {std:>4.2f}  \"\n",
    "              \"{min:>5.2f}  {q25:>5.2f}  {q50:>5.2f}  {q75:>5.2f}  {max:>5.2f}\"\n",
    "              .format(column_name_len=column_name_len,\n",
    "                      column_name=column_name,\n",
    "                      non_nan=sum(df[column_name].notnull()),\n",
    "                      mean=df[column_name].mean(),\n",
    "                      std=df[column_name].std(),\n",
    "                      min=df[column_name].min(),\n",
    "                      q25=df[column_name].quantile(0.25),\n",
    "                      q50=df[column_name].quantile(0.50),\n",
    "                      q75=df[column_name].quantile(0.75),\n",
    "                      max=max(df[column_name])))\n",
    "    print(\"\\n## Category Columns\")\n",
    "    print(\"{column_name:<{column_name_len}}: Non-nan   unique   top (count)  \"\n",
    "          \"rest\"\n",
    "          .format(column_name_len=column_name_len,\n",
    "                  column_name=\"Column name\"))\n",
    "    for column_name in column_info['category']:\n",
    "        # print(df[column_name].describe())\n",
    "        rest_str = str(column_info_meta[column_name]['value_list'][1:])[:40]\n",
    "        print(\"{column_name:<{column_name_len}}: {non_nan:>7}   {unique:>6}   \"\n",
    "              \"{top} ({count})  {rest}\"\n",
    "              .format(column_name_len=column_name_len,\n",
    "                      column_name=column_name,\n",
    "                      non_nan=sum(df[column_name].notnull()),\n",
    "                      unique=len(df[column_name].unique()),\n",
    "                      top=column_info_meta[column_name]['value_list'][0],\n",
    "                      count=column_info_meta[column_name]['top_count_val'],\n",
    "                      rest=rest_str))\n",
    "\n",
    "    print(\"\\n## Other Columns\")\n",
    "    print(\"{column_name:<{column_name_len}}: Non-nan   unique   top (count)\"\n",
    "          .format(column_name_len=column_name_len,\n",
    "                  column_name=\"Column name\"))\n",
    "    for column_name in column_info['other']:\n",
    "        rest_str = str(column_info_meta[column_name]['value_list'][1:])[:40]\n",
    "        print(\"{column_name:<{column_name_len}}: {non_nan:>7}   {unique:>6}   \"\n",
    "              \"{top} ({count})\"\n",
    "              .format(column_name_len=column_name_len,\n",
    "                      column_name=column_name,\n",
    "                      non_nan=sum(df[column_name].notnull()),\n",
    "                      unique=len(df[column_name].unique()),\n",
    "                      top=column_info_meta[column_name]['value_list'][0],\n",
    "                      count=column_info_meta[column_name]['top_count_val']))\n",
    "\n",
    "\n",
    "def get_parser():\n",
    "    \"\"\"Get parser object for exploratory data analysis.\"\"\"\n",
    "    from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n",
    "    parser = ArgumentParser(description=__doc__,\n",
    "                            formatter_class=ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument(\"-f\", \"--file\",\n",
    "                        dest=\"filename\",\n",
    "                        help=\"read this csv file\",\n",
    "                        metavar=\"FILE\",\n",
    "                        required=True)\n",
    "    return parser\n",
    "\n",
    "\n",
    "dtypes = {\n",
    "    'date': 'str',\n",
    "    'customer_created_at': 'str',\n",
    "    'customer_first_order_paid': 'str',\n",
    "    'is_coupon': 'str',\n",
    "    'is_campaign': 'str',\n",
    "    \n",
    "    'customer_id': 'category',\n",
    "    'customer_gender': 'category',\n",
    "    'channel_partner_name': 'category',\n",
    "    'sku_config': 'category',\n",
    "    'product_name': 'category',\n",
    "    'product_gender': 'category',\n",
    "    'product_color': 'category',\n",
    "    'product_brand': 'category',\n",
    "    'product_medium_image_url': 'category',\n",
    "    'cmc_category_bp': 'category', \n",
    "    'cmc_division_bp': 'category',\n",
    "    'cmc_business_unit_bp': 'category',\n",
    "    'google_product_category': 'category',\n",
    "    'payment_method_name': 'category',\n",
    "    'shipping_condition': 'category',\n",
    "    'device_name': 'category',\n",
    "    'platform_name': 'category',\n",
    "    'delivery_city': 'category',\n",
    "    'delivery_state_code': 'category',\n",
    "    'delivery_country_region': 'category',\n",
    "    'planning_cluster': 'category',\n",
    "    'planning_age': 'category',\n",
    "    'ticketrange_planning': 'category',\n",
    "    'originalpricerange_planning': 'category',\n",
    "    \n",
    "    'customer_age': 'int64',\n",
    "    'n_orders': 'int64',\n",
    "    \n",
    "    'sale_value': 'float64', \n",
    "    'product_original_price': 'float64',\n",
    "    'product_discount': 'float64',\n",
    "    'shipping_discount': 'float64',\n",
    "}\n",
    "parse_dates = ['date', 'customer_created_at', 'customer_first_order_paid']\n",
    "\n",
    "df = pd.read_csv(DATA_PATH, dtype=dtypes, parse_dates=parse_dates)\n",
    "\n",
    "describe_pandas_df(df.drop([\n",
    "    \"product_medium_image_url\", \"product_name\", \"n_orders\", \"customer_age\",\n",
    "    'date', 'customer_created_at', 'customer_first_order_paid'],\n",
    "    axis=1), dtype=dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of datapoints: 23664\n",
      "\n",
      "## Integer Columns\n",
      "Column name : Non-nan  mean   std   min   25%   50%   75%   max\n",
      "n_orders    :   23664  70.83  38.37    43    49    58    78   697\n",
      "customer_age:   23664  39.50  11.16     0    32    38    46   119\n",
      "\n",
      "## Float Columns\n",
      "Column name : Non-nan   mean    std    min    25%    50%    75%    max\n",
      "\n",
      "## Category Columns\n",
      "Column name : Non-nan   unique   top (count)  rest\n",
      "\n",
      "## Other Columns\n",
      "Column name : Non-nan   unique   top (count)\n"
     ]
    }
   ],
   "source": [
    "describe_pandas_df(df.drop_duplicates([\"customer_id\"], keep='first')[[\"n_orders\", \"customer_age\"]], dtype=dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
